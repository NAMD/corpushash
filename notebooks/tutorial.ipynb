{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> code {background-color : lightgrey !important;} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style> code {background-color : lightgrey !important;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## `hashcorpus` tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The `hashcorpus` library enables performing common NLP tasks on sensitive documents without disclosing their contents. This is done by hashing every token in the corpus along with a salt (to prevent dictionary attacks).\n",
    "\n",
    "The library requires as input:\n",
    "\n",
    "* a tokenized corpus as a nested list, whose elements are themselves nested lists of the tokens of each document in the corpus\n",
    "\n",
    "    each list corresponds to a document structure: its chapters, paragraphs, sentences. you decide how the nested list is to be created or structured, as long as the input is a nested list with strings as their bottom-most elements.\n",
    "\n",
    "* `corpus_path`, a path to a directory where the output files are to be stored\n",
    "\n",
    "The output includes:\n",
    "\n",
    "* a .json file for every item in the dictionary provided, named sequencially as positive integers, e.g., the first document being `0.json`, stored in `corpus_path/public/$(timestamp-of-hash)/`\n",
    "\n",
    "* two pickled dictionaries stored in `corpus_path/private`. they are used to decode the .json files or the NLP results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "loading libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.corpus import gutenberg\n",
    "import corpushash as ch\n",
    "import base64\n",
    "import hashlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we'll use the gutenberg corpus as test data, which is available through the nltk library.\n",
    "\n",
    "downloading test data (if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('gutenberg')  # comment (uncomment) if you have (don't have) the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "files in test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "creating test corpus path, where hashed documents will be stored as .json files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guest/Documents/git/hashed-nlp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = os.getcwd()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guest/Documents/git/hashed-nlp/guten_test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = os.path.join(base_path, 'guten_test')\n",
    "corpus_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### function to split text into nested list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period. \n"
     ]
    }
   ],
   "source": [
    "excerpt = gutenberg.raw('austen-emma.txt')[50:478]\n",
    "print(excerpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "every paragraph and sentence is its own list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home']], [['and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings']], [['of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world']], [['with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her']], [['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate']], [['indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', \"sister's\", 'marriage']], [['been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period']]]\n"
     ]
    }
   ],
   "source": [
    "print(ch.text_split(excerpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the library takes as input a nested list whose elements are the original documents as nested lists. this can be an in-nested list or some generator that yields a nested list when it is iterated over.\n",
    "\n",
    "#### creating nested list made from the raw texts in the gutenberg corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 22 ms, total: 1.13 s\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "guten_list = []\n",
    "for document_name in gutenberg.fileids():\n",
    "    document = gutenberg.raw(document_name)\n",
    "    split_document = ch.text_split(document)\n",
    "    guten_list.append(split_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "excerpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Persuasion', 'by', 'Jane', 'Austen', '1818']], [['Chapter', '1']], [['Sir', 'Walter', 'Elliot', 'of', 'Kellynch', 'Hall', 'in', 'Somersetshire', 'was', 'a', 'man', 'who']], [['for', 'his', 'own', 'amusement', 'never', 'took', 'up', 'any', 'book', 'but', 'the', 'Baronetage']], [['there', 'he', 'found', 'occupation', 'for', 'an', 'idle', 'hour', 'and', 'consolation', 'in', 'a']], [['distressed', 'one', 'there', 'his', 'faculties', 'were', 'roused', 'into', 'admiration', 'and']], [['respect', 'by', 'contemplating', 'the', 'limited', 'remnant', 'of', 'the', 'earliest', 'patents']], [['there', 'any', 'unwelcome', 'sensations', 'arising', 'from', 'domestic', 'affairs']], [['changed', 'naturally', 'into', 'pity', 'and', 'contempt', 'as', 'he', 'turned', 'over']], [['the', 'almost', 'endless', 'creations', 'of', 'the', 'last', 'century', 'and', 'there']]]\n"
     ]
    }
   ],
   "source": [
    "document = random.choice(guten_list)\n",
    "print(document[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## processing using `corpushash`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### instantiating `CorpusHash` class, which hashes the provided corpus to the `corpus_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dictionaries from previous hashing found. loading them.\n",
      "INFO:root:18 documents hashed and saved to /home/guest/Documents/git/hashed-nlp/guten_test/public/2017-05-03_18-48-49.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.28 s, sys: 114 ms, total: 6.39 s\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%time hashed_guten = ch.CorpusHash(guten_list, corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encode dictionary\n",
    "\n",
    "The encode dictionary is used to encode values to hashes, so that the same strings are guaranteed to be hashed to the same value, including the random salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token >>      endowment       | hashed_token >> 'H>tM?J1bYkuFd9#?v5@tO+74feW<f8PAkJRk+Be{'\n",
      "token >>       Blessed        | hashed_token >> '$0Qjsn8sFljaR<2GSbk39MFjb1j-ccN@H$v!s0M-'\n",
      "token >>       which--        | hashed_token >> 'sAPlSL3U0sI<87=772r{Fktzw#Mc9z^PfbKMC{QN'\n",
      "token >>        paste         | hashed_token >> 'PKQsysD4vL>lH>4E6K^{rdT}`?J+OhTUTsHnhhyH'\n",
      "token >>      thrashers       | hashed_token >> 'Mmnfz17U-dzhax0W&~P}T<gi&(`FZjdvULUo6Wr?'\n"
     ]
    }
   ],
   "source": [
    "entries = random.sample(list(hashed_guten.encode_dictionary.keys()), k=5)\n",
    "for entry in entries:\n",
    "    print(\"token >> {:^20} | hashed_token >> '{}'\".format(entry, hashed_guten.encode_dictionary[entry]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decode dictionary\n",
    "\n",
    "The decode dictionary is used to decode hashes to their original strings, so that one can make sense of the results of any posterior NLP analysis. It also lists the salt that was used to obtain the given hash. This dictionary must be kept secret with the owner of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashed_token >> 'w8t4S<2`PpQPasQOaop9LiW2(I$74nx8#~dSBeW=' | (token >> 'rout', salt >> 'b'\\xe9\\x86F\\xb3''\n",
      "hashed_token >> 'o8J*^aM0OG5Zd;P-xK!K8Vkq0$;?!-3`SI^7M$9^' | (token >> 'Darknesse', salt >> 'b'O\\x1c:\\x0c''\n",
      "hashed_token >> 'm8TcJZPk;F%`s7q<aZ%9SSKB1s0B^ZL7<NV@vI?&' | (token >> 'ayres', salt >> 'b'8\\x14\\xa8R''\n",
      "hashed_token >> 'a8?UumsR@P7T=0gZ1{l=_H?&$BWX^U6x|>9PhSh+' | (token >> 'foremen', salt >> 'b'\\xc8\\x82\\xb1\\x14''\n",
      "hashed_token >> 'o1?QpYJrk?59R~l4v55I?ER0RkOIQBxMiearEH~$' | (token >> 'God-enfranchis'd', salt >> 'b'9\\xcaj\\x15''\n"
     ]
    }
   ],
   "source": [
    "entries = random.sample(list(hashed_guten.decode_dictionary.keys()), k=5)\n",
    "for entry in entries:\n",
    "    print(\"hashed_token >> '{}' | (token >> '{}', salt >> '{}'\".format(entry, hashed_guten.decode_dictionary[entry][0], hashed_guten.decode_dictionary[entry][1][:4]))  # cutting off some bytes for aesthetic reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### hashed .json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the `walk_nested_list` function yields items in a nested list in order, regardless of their depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period. \n"
     ]
    }
   ],
   "source": [
    "print(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma\n",
      "Woodhouse\n",
      "handsome\n",
      "clever\n",
      "and\n",
      "rich\n",
      "with\n",
      "a\n",
      "comfortable\n",
      "home\n",
      "and\n",
      "happy\n",
      "disposition\n",
      "seemed\n",
      "to\n",
      "unite\n",
      "some\n",
      "of\n",
      "the\n",
      "best\n",
      "blessings\n",
      "of\n",
      "existence\n",
      "and\n",
      "had\n",
      "lived\n",
      "nearly\n",
      "twenty-one\n",
      "years\n",
      "in\n",
      "the\n",
      "world\n",
      "with\n",
      "very\n",
      "little\n",
      "to\n",
      "distress\n",
      "or\n",
      "vex\n",
      "her\n",
      "She\n",
      "was\n",
      "the\n",
      "youngest\n",
      "of\n",
      "the\n",
      "two\n",
      "daughters\n",
      "of\n",
      "a\n",
      "most\n",
      "affectionate\n",
      "indulgent\n",
      "father\n",
      "and\n",
      "had\n",
      "in\n",
      "consequence\n",
      "of\n",
      "her\n",
      "sister's\n",
      "marriage\n",
      "been\n",
      "mistress\n",
      "of\n",
      "his\n",
      "house\n",
      "from\n",
      "a\n",
      "very\n",
      "early\n",
      "period\n"
     ]
    }
   ],
   "source": [
    "for element in ch.walk_nested_list(ch.text_split(excerpt)):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### we can use this function to see what hashcodecs has done to the corpus.\n",
    "\n",
    "adjusting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 13 corresponds to milton-paradise.txt.\n"
     ]
    }
   ],
   "source": [
    "limit = 10  # showing first ten entries\n",
    "document = random.randint(0, len(gutenberg.fileids()))\n",
    "print('document {} corresponds to {}.'.format(document, gutenberg.fileids()[document]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**note**: take care to have your corpus yield the documents always in the same order, else you'll have a harder time knowing which hashed documents correspond to which documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "reading output .json as a nested list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "document_path = os.path.join(hashed_guten.public_path, \"{}.json\".format(document))\n",
    "with open(document_path, mode=\"rt\") as fp:\n",
    "    encoded_document = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original token >> encoded token\n",
      "'Paradise' >> 'E{9&7z%En+)8vO?$5C$);e3%QIn^7#2grDHGp8m{'\n",
      "'Lost' >> 'l_Z6-kd~m;D&`WBVNoXiaC)f%nhzfn!ym>O*NdQR'\n",
      "'by' >> 'RF#n3YIWY99SilI=6|%A7{kxOG!PAmR6b_^l|pmM'\n",
      "'John' >> 'J<uL5e0VMjgK2XOg6nzisq|A>Vs4^Eivwi}E`v@6'\n",
      "'Milton' >> 'U13f5tX+fe2n6b~U4AbMLyIDH1!hDa<M-xICww!>'\n",
      "'1667' >> 'j7bZ4ME;O5o-bgKEhX4g@RQcE`bYvl6zGh#kP)a)'\n",
      "'Book' >> 'a!_YTTzXY!CHK4!q>x!6Fz+HK&ZE|M!Zv(1yc1{S'\n",
      "'I' >> '>-^j%YNz_lr{|A_&MnGv95)G)8TETmZhND{*ET#n'\n",
      "'Of' >> 'wsBz1B>a^hUeX>GZvJJV;5F;)5$!DOM}Yc;bm<yD'\n",
      "'Man's' >> 'EINDPhg<|Ehf%*WeiOR%B2eI+jhu|1?*y~w5|NN&'\n",
      "'first' >> '*Wbi}j`5YHU0X!jD&Ojr=^$)h30TCN!{9f8ZoN3n'\n",
      "'disobedience' >> '>U!C}R!}8QFpYX`F@$MR(F(T+9Cu1kd6{%?DM+$?'\n"
     ]
    }
   ],
   "source": [
    "print(\"original token >> encoded token\")\n",
    "for ix, tokens in enumerate(zip(ch.walk_nested_list(guten_list[document]), ch.walk_nested_list(encoded_document))):\n",
    "    print(\"'{}' >> '{}'\".format(tokens[0], tokens[1]))\n",
    "    if ix > limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "alternatively, one can check the `corpus_path` directory and read the output files using one's favorite text editor.\n",
    "\n",
    "the path is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guest/Documents/git/hashed-nlp/guten_test/public/2017-05-03_18-48-49'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_guten.public_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### how this library works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "this is the basic algorithm for the `corpushash` library. for more details, check the source code, it is readable.\n",
    "\n",
    "- Create an empty corpus structure (nested list) to hold the hashed tokens;\n",
    "- Create a decoding dictionary: a list of key, value pairs where the key is an encoded token (hash) and the values are the unhashed token and its salt.\n",
    "- Create an encoding dictionary: a list of key, value pairs where the key is a plain token and the value is its cryptographic hash.\n",
    "- Iterate over the unhashed tokens\n",
    "    - Check if the word is in the encoding dictionary;\n",
    "    - If so, add its hash value to the hashed tokens list\n",
    "    - If not, hash it with the addition of a random salt, and add them to the encoding and decoding dictionaries.\n",
    "- Return the hashed corpus and the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### optional arguments\n",
    "\n",
    "`hash_function`: default is sha-256, but can be any hash function offered by the hashlib library that does not need additional parameters (as does scrypt, for instance)\n",
    "`salt_length`: determines salt length in bytes, default is 32 bytes.\n",
    "`one_salt`: determines if tokens will be hashed with the same salt or one for each token. if True, os.urandom generates a salt to be used in all hashings. (**note**: in this case, choose a greater salt length). if False, os.urandom will generate a salt for each token.\n",
    "`encoding`: determines the encoding of the outputted .json files. default is utf-8, and you probably want to keep it that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### hashing documents from the same corpus at different times\n",
    "\n",
    "hashing documents from the same corpus at different times is supported in case you produce documents continuously.\n",
    "\n",
    "by specifying a `corpus_path` from a previous instance of `CorpusHash` to this new instance, it'll automatically search for and employ the same dictionaries used in the last hashing, which means the same tokens in the old and new documents will map to the same hash.\n",
    "\n",
    "the files will be saved to a new directory in the `public/` directory, named after the timestamp of this instance of `CorpusHash`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**note**: be careful when specifying a previously used `corpus_path` if the above is not what you want to do. if you want new dictionaries, just specify a new `corpus_path`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**note**: when specifying a previously used `corpus_path`, take care with the optional arguments of `CorpusHash`. \n",
    "\n",
    "- specifying a different `hash_function` will cause the hashes of the same words in each instance to differ. \n",
    "\n",
    "- if you set `one_salt` to `True`, the library will assume this was also `True` for the previous instances of `CorpusHash`, and will take an arbitrary salt from the `decode_dictionary` as the salt to be used in this instance -- they should all be the same, after all. if `one_salt` was not `True` for the previous instances, this will produce unexpected results.\n",
    "    - additionally, if you pass `one_salt=True` in this situation with any value to `salt_length`, this value will be ignored.\n",
    "    - if you pass `one_salt=True` in this situation with a different value than the last to `hash_function`, any token not hashed in the previous instances will be hashed with a different hash function, which may mean a different hash length, for example. the tokens that were hashed in the previous instances will take the same value as before, because they are looked up in the `encode_dictionary` and not re-hashed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
