{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clustering the twitter samples corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**corpushash** is a simple library that aims to make the natural language processing of sensitive documents easier. the library enables performing common NLP tasks on sensitive documents without disclosing their contents. This is done by hashing every token in the corpus along with a salt (to prevent dictionary attacks). \n",
    "\n",
    "its workflow is as simple as having the sensitive corpora as a python nested list (or generator) whose elements are themselves (nested) lists of strings. after the hashing is done, NLP can be carried out by a third party, and when the results are in they can be decoded by a dictionary that maps hashes to the original strings. so that makes:\n",
    "\n",
    "```python\n",
    "import corpushash as ch\n",
    "hashed_corpus = ch.CorpusHash(mycorpus_as_a_nested_list, '/home/sensitive-corpus')\n",
    ">>> \"42 documents hashed and saved to '/home/sensitive-corpus/public/$(timestamp)'\"\n",
    "```\n",
    "**NLP is done, and `results` are in**:\n",
    "```python\n",
    "for token in results:\n",
    "    print(token, \">\", hashed_corpus.decode_dictionary[token])\n",
    ">>> \"7)JBMGG?sGu+>%Js~dG=%c1Qn1HpAU{jM-~Buu7?\" > \"gutenberg\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging, bz2, os\n",
    "from corpushash import CorpusHash\n",
    "from nltk.corpus import twitter_samples as tt\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### uncomment this if you don't have the corpus downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "specify the directory you'd like to save files to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "this is needed because `gensim`'s `doc2bow` has some random behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### the twitter samples corpus\n",
    "\n",
    "this is how the corpus looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(',\n",
       " \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\",\n",
       " '@RileyMcDonough make me smile :((',\n",
       " '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln',\n",
       " 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"',\n",
       " 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.strings()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "but we'll be using the pre-tokenized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless', 'for', 'tmr', ':(']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tokenized()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt.tokenized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoded_twitter = tt.tokenized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### building gensim dictionary\n",
    "\n",
    "from this document generator gensim will build a dictionary that maps every hashed token to an ID, a mapping which is later used to calculate the tf-idf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:43,589 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-05-23 19:20:43,775 : INFO : adding document #10000 to Dictionary(24343 unique tokens: ['hopeless', 'for', 'tmr', ':(', 'Everything']...)\n",
      "2017-05-23 19:20:44,054 : INFO : adding document #20000 to Dictionary(35614 unique tokens: ['hopeless', 'for', 'tmr', ':(', 'Everything']...)\n",
      "2017-05-23 19:20:44,343 : INFO : built Dictionary(42532 unique tokens: ['hopeless', 'for', 'tmr', ':(', 'Everything']...) from 30000 documents (total 580322 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary(decoded_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hopeless'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### bag-of-words\n",
    "\n",
    "to build a tf-idf model, the gensim library needs an input that yields this vectorized bag-of-words when iterated over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:44,984 : INFO : storing corpus in Matrix Market format to /home/guest/Documents/git/corpushash/twitter_pt_tfidf.mm\n",
      "2017-05-23 19:20:44,987 : INFO : saving sparse matrix to /home/guest/Documents/git/corpushash/twitter_pt_tfidf.mm\n",
      "2017-05-23 19:20:44,987 : INFO : PROGRESS: saving document #0\n",
      "2017-05-23 19:20:45,005 : INFO : PROGRESS: saving document #1000\n",
      "2017-05-23 19:20:45,023 : INFO : PROGRESS: saving document #2000\n",
      "2017-05-23 19:20:45,041 : INFO : PROGRESS: saving document #3000\n",
      "2017-05-23 19:20:45,058 : INFO : PROGRESS: saving document #4000\n",
      "2017-05-23 19:20:45,076 : INFO : PROGRESS: saving document #5000\n",
      "2017-05-23 19:20:45,094 : INFO : PROGRESS: saving document #6000\n",
      "2017-05-23 19:20:45,112 : INFO : PROGRESS: saving document #7000\n",
      "2017-05-23 19:20:45,130 : INFO : PROGRESS: saving document #8000\n",
      "2017-05-23 19:20:45,149 : INFO : PROGRESS: saving document #9000\n",
      "2017-05-23 19:20:45,168 : INFO : PROGRESS: saving document #10000\n",
      "2017-05-23 19:20:45,198 : INFO : PROGRESS: saving document #11000\n",
      "2017-05-23 19:20:45,230 : INFO : PROGRESS: saving document #12000\n",
      "2017-05-23 19:20:45,264 : INFO : PROGRESS: saving document #13000\n",
      "2017-05-23 19:20:45,296 : INFO : PROGRESS: saving document #14000\n",
      "2017-05-23 19:20:45,326 : INFO : PROGRESS: saving document #15000\n",
      "2017-05-23 19:20:45,355 : INFO : PROGRESS: saving document #16000\n",
      "2017-05-23 19:20:45,386 : INFO : PROGRESS: saving document #17000\n",
      "2017-05-23 19:20:45,415 : INFO : PROGRESS: saving document #18000\n",
      "2017-05-23 19:20:45,444 : INFO : PROGRESS: saving document #19000\n",
      "2017-05-23 19:20:45,473 : INFO : PROGRESS: saving document #20000\n",
      "2017-05-23 19:20:45,501 : INFO : PROGRESS: saving document #21000\n",
      "2017-05-23 19:20:45,531 : INFO : PROGRESS: saving document #22000\n",
      "2017-05-23 19:20:45,560 : INFO : PROGRESS: saving document #23000\n",
      "2017-05-23 19:20:45,588 : INFO : PROGRESS: saving document #24000\n",
      "2017-05-23 19:20:45,617 : INFO : PROGRESS: saving document #25000\n",
      "2017-05-23 19:20:45,646 : INFO : PROGRESS: saving document #26000\n",
      "2017-05-23 19:20:45,675 : INFO : PROGRESS: saving document #27000\n",
      "2017-05-23 19:20:45,704 : INFO : PROGRESS: saving document #28000\n",
      "2017-05-23 19:20:45,733 : INFO : PROGRESS: saving document #29000\n",
      "2017-05-23 19:20:45,762 : INFO : saved 30000x42532 matrix, density=0.042% (538552/1275960000)\n",
      "2017-05-23 19:20:45,763 : INFO : saving MmCorpus index to /home/guest/Documents/git/corpushash/twitter_pt_tfidf.mm.index\n"
     ]
    }
   ],
   "source": [
    "mm = [id2word.doc2bow(text) for text in decoded_twitter]\n",
    "gensim.corpora.MmCorpus.serialize(os.path.join(path, 'twitter_pt_tfidf.mm'), mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:45,769 : INFO : collecting document frequencies\n",
      "2017-05-23 19:20:45,771 : INFO : PROGRESS: processing document #0\n",
      "2017-05-23 19:20:45,800 : INFO : PROGRESS: processing document #10000\n",
      "2017-05-23 19:20:45,851 : INFO : PROGRESS: processing document #20000\n",
      "2017-05-23 19:20:45,898 : INFO : calculating IDF weights for 30000 documents and 42531 features (538552 matrix non-zeros)\n",
      "2017-05-23 19:20:45,919 : INFO : saving TfidfModel object under twitter_tfidf_model, separately None\n",
      "2017-05-23 19:20:45,926 : INFO : saved twitter_tfidf_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 ms, sys: 2 ms, total: 155 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(os.path.join(path, 'twitter_tfidf_model')):\n",
    "    tfidf = models.TfidfModel.load(os.path.join(path, 'twitter_tfidf_model'))\n",
    "else:\n",
    "    tfidf = models.TfidfModel(mm)\n",
    "    tfidf.save('twitter_tfidf_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Calculating the LSI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next step is to train the LSI model with a tfidf transformed corpus. So we will need yet another generator to yield the transformed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tfidf_corpus_stream(corpus):\n",
    "    for doc in corpus:\n",
    "        yield tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_corpus_s = tfidf_corpus_stream(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:46,307 : INFO : using serial LSI version on this node\n",
      "2017-05-23 19:20:46,308 : INFO : updating model with new documents\n",
      "2017-05-23 19:20:46,722 : INFO : preparing a new chunk of documents\n",
      "2017-05-23 19:20:46,818 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-05-23 19:20:46,819 : INFO : 1st phase: constructing (42532, 200) action matrix\n",
      "2017-05-23 19:20:47,034 : INFO : orthonormalizing (42532, 200) action matrix\n",
      "2017-05-23 19:20:49,605 : INFO : 2nd phase: running dense svd on (200, 20000) matrix\n",
      "2017-05-23 19:20:50,298 : INFO : computing the final decomposition\n",
      "2017-05-23 19:20:50,299 : INFO : keeping 100 factors (discarding 22.970% of energy spectrum)\n",
      "2017-05-23 19:20:50,384 : INFO : processed documents up to #20000\n",
      "2017-05-23 19:20:50,387 : INFO : topic #0(20.207): 0.246*\"\"\" + 0.207*\"SNP\" + 0.186*\"Tories\" + 0.174*\"Miliband\" + 0.171*\"in\" + 0.162*\"is\" + 0.160*\"to\" + 0.154*\"of\" + 0.146*\"Sco\" + 0.146*\"…\"\n",
      "2017-05-23 19:20:50,389 : INFO : topic #1(17.533): 0.288*\"\"\" + -0.200*\"SNP\" + 0.187*\"preoccupied\" + 0.187*\"@Tommy_Colc\" + 0.187*\"inequality\" + 0.186*\"wrote\" + 0.179*\"claiming\" + 0.177*\"man\" + 0.174*\"come\" + 0.174*\"w\"\n",
      "2017-05-23 19:20:50,392 : INFO : topic #2(14.004): 0.235*\"(\" + 0.209*\"%\" + 0.200*\":(\" + 0.189*\"I\" + 0.174*\"-\" + 0.170*\"!\" + 0.167*\"you\" + 0.163*\"the\" + 0.147*\")\" + 0.145*\"a\"\n",
      "2017-05-23 19:20:50,394 : INFO : topic #3(12.137): 0.543*\"%\" + 0.331*\"-\" + 0.317*\"(\" + 0.251*\")\" + 0.223*\"1\" + 0.124*\"+\" + 0.116*\"CON\" + 0.115*\"LAB\" + -0.114*\"I\" + -0.113*\"you\"\n",
      "2017-05-23 19:20:50,397 : INFO : topic #4(10.230): -0.424*\":(\" + -0.378*\"(\" + 0.216*\"%\" + -0.204*\"!\" + 0.182*\"'\" + -0.160*\"i\" + -0.158*\":)\" + 0.126*\":\" + -0.119*\"I\" + 0.117*\"Cameron\"\n",
      "2017-05-23 19:20:50,754 : INFO : preparing a new chunk of documents\n",
      "2017-05-23 19:20:50,809 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-05-23 19:20:50,810 : INFO : 1st phase: constructing (42532, 200) action matrix\n",
      "2017-05-23 19:20:50,933 : INFO : orthonormalizing (42532, 200) action matrix\n",
      "2017-05-23 19:20:53,748 : INFO : 2nd phase: running dense svd on (200, 10000) matrix\n",
      "2017-05-23 19:20:54,048 : INFO : computing the final decomposition\n",
      "2017-05-23 19:20:54,050 : INFO : keeping 100 factors (discarding 23.851% of energy spectrum)\n",
      "2017-05-23 19:20:54,109 : INFO : merging projections: (42532, 100) + (42532, 100)\n",
      "2017-05-23 19:20:54,728 : INFO : keeping 100 factors (discarding 12.573% of energy spectrum)\n",
      "2017-05-23 19:20:54,825 : INFO : processed documents up to #30000\n",
      "2017-05-23 19:20:54,828 : INFO : topic #0(26.147): 0.313*\"\"\" + 0.181*\"Tories\" + 0.171*\"preoccupied\" + 0.171*\"inequality\" + 0.171*\"@Tommy_Colc\" + 0.171*\"wrote\" + 0.167*\"Miliband\" + 0.167*\"claiming\" + 0.166*\"w\" + 0.164*\"man\"\n",
      "2017-05-23 19:20:54,831 : INFO : topic #1(22.136): 0.247*\"SNP\" + -0.210*\"\"\" + 0.178*\"Sco\" + 0.177*\"to\" + 0.176*\"protect\" + 0.176*\"lots\" + 0.175*\"definitely\" + 0.172*\"@NicolaSturgeon\" + 0.172*\"rather\" + 0.170*\"let\"\n",
      "2017-05-23 19:20:54,834 : INFO : topic #2(17.285): 0.194*\"the\" + 0.175*\".\" + -0.169*\"protect\" + -0.169*\"lots\" + -0.168*\"definitely\" + -0.168*\"Sco\" + 0.159*\"a\" + 0.159*\"%\" + 0.148*\"I\" + -0.147*\"MPs\"\n",
      "2017-05-23 19:20:54,837 : INFO : topic #3(14.181): -0.646*\"%\" + -0.319*\"-\" + -0.266*\"(\" + -0.220*\")\" + -0.195*\"1\" + -0.131*\"CON\" + -0.130*\"LAB\" + -0.127*\"poll\" + -0.126*\"8\" + -0.125*\"34\"\n",
      "2017-05-23 19:20:54,840 : INFO : topic #4(11.743): -0.236*\"thus\" + -0.236*\"ahem\" + -0.236*\"@thomasmessenger\" + -0.236*\"http://t.co/DkLwCwzhDA\" + -0.235*\"financial\" + -0.234*\"caused\" + -0.233*\"global\" + -0.233*\"crisis\" + -0.225*\"For\" + -0.225*\"overspent\"\n",
      "2017-05-23 19:20:54,869 : INFO : saving Projection object under /home/guest/Documents/git/corpushash/twitter_lsi_model.projection, separately None\n",
      "2017-05-23 19:20:55,064 : INFO : saved /home/guest/Documents/git/corpushash/twitter_lsi_model.projection\n",
      "2017-05-23 19:20:55,066 : INFO : saving LsiModel object under /home/guest/Documents/git/corpushash/twitter_lsi_model, separately None\n",
      "2017-05-23 19:20:55,066 : INFO : not storing attribute projection\n",
      "2017-05-23 19:20:55,067 : INFO : not storing attribute dispatcher\n",
      "2017-05-23 19:20:55,084 : INFO : saved /home/guest/Documents/git/corpushash/twitter_lsi_model\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(path, 'twitter_lsi_model')):\n",
    "    lsi = gensim.models.LsiModel.load(os.path.join(path, 'twitter_lsi_model'))\n",
    "else:\n",
    "    lsi = gensim.models.lsimodel.LsiModel(corpus=tfidf_corpus_s, id2word=id2word, num_topics=100)\n",
    "    lsi.save(os.path.join(path, 'twitter_lsi_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Topic 0:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.313\t\"\n",
      "0.181\tTories\n",
      "0.171\tpreoccupied\n",
      "0.171\tinequality\n",
      "0.171\t@Tommy_Colc\n",
      "0.171\twrote\n",
      "0.167\tMiliband\n",
      "0.167\tclaiming\n",
      "0.166\tw\n",
      "0.164\tman\n",
      "====================\n",
      "Topic 1:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.247\tSNP\n",
      "-0.21\t\"\n",
      "0.178\tSco\n",
      "0.177\tto\n",
      "0.176\tprotect\n",
      "0.176\tlots\n",
      "0.175\tdefinitely\n",
      "0.172\t@NicolaSturgeon\n",
      "0.172\trather\n",
      "0.17\tlet\n",
      "====================\n",
      "Topic 2:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.194\tthe\n",
      "0.175\t.\n",
      "-0.169\tprotect\n",
      "-0.169\tlots\n",
      "-0.168\tdefinitely\n",
      "-0.168\tSco\n",
      "0.159\ta\n",
      "0.159\t%\n",
      "0.148\tI\n",
      "-0.147\tMPs\n",
      "====================\n",
      "Topic 3:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.646\t%\n",
      "-0.319\t-\n",
      "-0.266\t(\n",
      "-0.22\t)\n",
      "-0.195\t1\n",
      "-0.131\tCON\n",
      "-0.13\tLAB\n",
      "-0.127\tpoll\n",
      "-0.126\t8\n",
      "-0.125\t34\n",
      "====================\n",
      "Topic 4:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.236\tthus\n",
      "-0.236\tahem\n",
      "-0.236\t@thomasmessenger\n",
      "-0.236\thttp://t.co/DkLwCwzhDA\n",
      "-0.235\tfinancial\n",
      "-0.234\tcaused\n",
      "-0.233\tglobal\n",
      "-0.233\tcrisis\n",
      "-0.225\tFor\n",
      "-0.225\toverspent\n",
      "====================\n",
      "Topic 5:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.341\tFT\n",
      "0.321\t(\n",
      "-0.232\t%\n",
      "0.2\t)\n",
      "0.19\t:(\n",
      "0.177\tJonathan\n",
      "0.177\tFord\n",
      "0.177\twriter\n",
      "0.176\tBoris\n",
      "-0.162\t'\n",
      "====================\n",
      "Topic 6:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.414\t'\n",
      "0.182\tdeal\n",
      "-0.171\tCameron\n",
      "-0.162\tDavid\n",
      "0.152\tTomorrow\n",
      "0.147\tmyself\n",
      "0.147\t@mrmarksteel\n",
      "0.145\tcase\n",
      "-0.141\ton\n",
      "0.136\ttell\n",
      "====================\n",
      "Topic 7:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.282\t!\n",
      "0.279\t:(\n",
      "-0.232\tFT\n",
      "0.187\t:)\n",
      "0.182\tyou\n",
      "0.171\tI\n",
      "-0.155\tleader\n",
      "-0.127\t'\n",
      "-0.123\tFord\n",
      "-0.123\tJonathan\n",
      "====================\n",
      "Topic 8:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.312\t'\n",
      "0.24\t-\n",
      "0.21\t(\n",
      "0.197\tLabour\n",
      "-0.173\t,\n",
      "-0.162\t%\n",
      "0.146\t\"\n",
      "0.133\tSNP\n",
      "0.13\twith\n",
      "-0.13\tFT\n",
      "====================\n",
      "Topic 9:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.365\t(\n",
      "0.264\tDavid\n",
      "0.248\tCameron\n",
      "0.202\t:(\n",
      "-0.181\t%\n",
      "0.151\t...\n",
      "-0.144\t#AskNigelFarage\n",
      "0.137\t-\n",
      "0.13\t)\n",
      "0.129\t*\n",
      "====================\n",
      "Topic 10:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.356\t!\n",
      "-0.224\tyou\n",
      "-0.198\the'd\n",
      "-0.185\tthan\n",
      "0.182\t\"\n",
      "-0.182\trather\n",
      "-0.171\t:)\n",
      "0.153\tno\n",
      "-0.139\tlet\n",
      "0.134\t(\n",
      "====================\n",
      "Topic 11:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.527\t\"\n",
      "-0.367\t!\n",
      "0.218\t:(\n",
      "-0.134\t:)\n",
      "0.129\tI\n",
      "0.126\t.\n",
      "-0.121\t-\n",
      "0.0924\t(\n",
      "0.0896\tTories\n",
      "-0.0893\t'\n",
      "====================\n",
      "Topic 12:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.394\t'\n",
      "-0.262\tretweet\n",
      "-0.198\tnot\n",
      "-0.194\tthis\n",
      "-0.165\tdo\n",
      "0.163\t(\n",
      "-0.156\t@LabourEoin\n",
      "-0.138\twould\n",
      "-0.133\thttp://t.co/5D2pKCstr3\n",
      "-0.131\trepeat\n",
      "====================\n",
      "Topic 13:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.527\t\"\n",
      "-0.278\t!\n",
      "-0.201\t'\n",
      "-0.156\t-\n",
      "0.15\tnot\n",
      "-0.143\thttp\n",
      "-0.126\t*\n",
      "0.115\tsays\n",
      "0.106\tdo\n",
      "0.103\tthan\n",
      "====================\n",
      "Topic 14:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.186\treliant\n",
      "-0.186\thungrier\n",
      "0.184\tEd\n",
      "-0.182\tfive\n",
      "-0.178\tbanks\n",
      "-0.176\tfood\n",
      "-0.176\t@Markfergusonuk\n",
      "0.174\t*\n",
      "-0.17\tago\n",
      "-0.164\tyears\n",
      "====================\n",
      "Topic 15:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.26\t#AskNigelFarage\n",
      "-0.206\tFarage\n",
      "0.198\t\"\n",
      "-0.18\t(\n",
      "-0.18\tretweet\n",
      "-0.162\tNigel\n",
      "-0.16\t@Nigel_Farage\n",
      "-0.16\t@UKIP\n",
      "0.157\t:)\n",
      "-0.155\t#UKIP\n",
      "====================\n",
      "Topic 16:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.237\t:(\n",
      "0.162\tEd\n",
      "-0.161\t-\n",
      "-0.16\t)\n",
      "0.144\t%\n",
      "-0.14\twill\n",
      "-0.137\t#bbcqt\n",
      "-0.134\t#AskNigelFarage\n",
      "-0.131\tchild\n",
      "-0.13\t*\n"
     ]
    }
   ],
   "source": [
    "for n in range(17):\n",
    "    print(\"====================\")\n",
    "    print(\"Topic {}:\".format(n))\n",
    "    print(\"Coef.\\t Token\")\n",
    "    print(\"--------------------\")\n",
    "    for tok,coef in lsi.show_topic(n):\n",
    "        print(\"{:.3}\\t{}\".format(coef,tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSI on the hashed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now all of the original documents have been hashed, and we can run the same analysis we ran with the plain corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## processing using the `corpushash` library\n",
    "\n",
    "#### instatiating CorpusHash class, which hashes the provided corpus to the corpus_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:55,158 - corpushash.hashers - INFO - dictionaries from previous hashing found. loading them.\n",
      "2017-05-23 19:20:55,158 : INFO : dictionaries from previous hashing found. loading them.\n",
      "2017-05-23 19:20:59,462 - corpushash.hashers - INFO - 30000 documents hashed and saved to twitter/public/2017-05-23_19-20-55-158049.\n",
      "2017-05-23 19:20:59,462 : INFO : 30000 documents hashed and saved to twitter/public/2017-05-23_19-20-55-158049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 813 ms, total: 3.37 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashed = CorpusHash(decoded_twitter, 'twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "that is it. `corpushash`'s work is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### building dictionary for the hashed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:20:59,480 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-05-23 19:21:00,100 : INFO : adding document #10000 to Dictionary(24343 unique tokens: ['U+)Z~DkL&Y*tv9|JE+WuIku3sqrEkbkcx@_!*{DS', 'OKi%KcU3pFH?GQV=dMytpb=FXANJR4+aX|-;T%#G', 'F#lHfIZmtjgo_S)N$r411J8#X%Veb(%r8qSY__7(', 'j<oOL8!FI2kXUr!C&u}#q#ipUM4pq%dbICGr(A8%', '8=wMA2$wf<p}Y|by;)*c7<9lUz3N?LKiGV4Y7Tld']...)\n",
      "2017-05-23 19:21:00,835 : INFO : adding document #20000 to Dictionary(35614 unique tokens: ['U+)Z~DkL&Y*tv9|JE+WuIku3sqrEkbkcx@_!*{DS', 'OKi%KcU3pFH?GQV=dMytpb=FXANJR4+aX|-;T%#G', 'F#lHfIZmtjgo_S)N$r411J8#X%Veb(%r8qSY__7(', 'j<oOL8!FI2kXUr!C&u}#q#ipUM4pq%dbICGr(A8%', '8=wMA2$wf<p}Y|by;)*c7<9lUz3N?LKiGV4Y7Tld']...)\n",
      "2017-05-23 19:21:01,562 : INFO : built Dictionary(42532 unique tokens: ['U+)Z~DkL&Y*tv9|JE+WuIku3sqrEkbkcx@_!*{DS', 'OKi%KcU3pFH?GQV=dMytpb=FXANJR4+aX|-;T%#G', 'F#lHfIZmtjgo_S)N$r411J8#X%Veb(%r8qSY__7(', 'j<oOL8!FI2kXUr!C&u}#q#ipUM4pq%dbICGr(A8%', '8=wMA2$wf<p}Y|by;)*c7<9lUz3N?LKiGV4Y7Tld']...) from 30000 documents (total 580322 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary(hashed.read_hashed_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U+)Z~DkL&Y*tv9|JE+WuIku3sqrEkbkcx@_!*{DS'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:22:06,911 : INFO : storing corpus in Matrix Market format to /home/guest/Documents/git/corpushash/hashed_twitter_pt_tfidf.mm\n",
      "2017-05-23 19:22:07,342 : INFO : saving sparse matrix to /home/guest/Documents/git/corpushash/hashed_twitter_pt_tfidf.mm\n",
      "2017-05-23 19:22:07,344 : INFO : PROGRESS: saving document #0\n",
      "2017-05-23 19:22:07,362 : INFO : PROGRESS: saving document #1000\n",
      "2017-05-23 19:22:07,381 : INFO : PROGRESS: saving document #2000\n",
      "2017-05-23 19:22:07,398 : INFO : PROGRESS: saving document #3000\n",
      "2017-05-23 19:22:07,416 : INFO : PROGRESS: saving document #4000\n",
      "2017-05-23 19:22:07,433 : INFO : PROGRESS: saving document #5000\n",
      "2017-05-23 19:22:07,451 : INFO : PROGRESS: saving document #6000\n",
      "2017-05-23 19:22:07,470 : INFO : PROGRESS: saving document #7000\n",
      "2017-05-23 19:22:07,489 : INFO : PROGRESS: saving document #8000\n",
      "2017-05-23 19:22:07,507 : INFO : PROGRESS: saving document #9000\n",
      "2017-05-23 19:22:07,526 : INFO : PROGRESS: saving document #10000\n",
      "2017-05-23 19:22:07,557 : INFO : PROGRESS: saving document #11000\n",
      "2017-05-23 19:22:07,588 : INFO : PROGRESS: saving document #12000\n",
      "2017-05-23 19:22:07,619 : INFO : PROGRESS: saving document #13000\n",
      "2017-05-23 19:22:07,649 : INFO : PROGRESS: saving document #14000\n",
      "2017-05-23 19:22:07,680 : INFO : PROGRESS: saving document #15000\n",
      "2017-05-23 19:22:07,710 : INFO : PROGRESS: saving document #16000\n",
      "2017-05-23 19:22:07,742 : INFO : PROGRESS: saving document #17000\n",
      "2017-05-23 19:22:07,774 : INFO : PROGRESS: saving document #18000\n",
      "2017-05-23 19:22:07,806 : INFO : PROGRESS: saving document #19000\n",
      "2017-05-23 19:22:07,838 : INFO : PROGRESS: saving document #20000\n",
      "2017-05-23 19:22:07,870 : INFO : PROGRESS: saving document #21000\n",
      "2017-05-23 19:22:07,903 : INFO : PROGRESS: saving document #22000\n",
      "2017-05-23 19:22:07,935 : INFO : PROGRESS: saving document #23000\n",
      "2017-05-23 19:22:07,968 : INFO : PROGRESS: saving document #24000\n",
      "2017-05-23 19:22:07,998 : INFO : PROGRESS: saving document #25000\n",
      "2017-05-23 19:22:08,028 : INFO : PROGRESS: saving document #26000\n",
      "2017-05-23 19:22:08,059 : INFO : PROGRESS: saving document #27000\n",
      "2017-05-23 19:22:08,089 : INFO : PROGRESS: saving document #28000\n",
      "2017-05-23 19:22:08,119 : INFO : PROGRESS: saving document #29000\n",
      "2017-05-23 19:22:08,149 : INFO : saved 30000x42532 matrix, density=0.042% (538552/1275960000)\n",
      "2017-05-23 19:22:08,150 : INFO : saving MmCorpus index to /home/guest/Documents/git/corpushash/hashed_twitter_pt_tfidf.mm.index\n"
     ]
    }
   ],
   "source": [
    "mm = [id2word.doc2bow(text) for text in hashed.read_hashed_corpus()]\n",
    "gensim.corpora.MmCorpus.serialize(os.path.join(path, 'hashed_twitter_pt_tfidf.mm'), mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:22:08,157 : INFO : collecting document frequencies\n",
      "2017-05-23 19:22:08,159 : INFO : PROGRESS: processing document #0\n",
      "2017-05-23 19:22:08,189 : INFO : PROGRESS: processing document #10000\n",
      "2017-05-23 19:22:08,237 : INFO : PROGRESS: processing document #20000\n",
      "2017-05-23 19:22:08,284 : INFO : calculating IDF weights for 30000 documents and 42531 features (538552 matrix non-zeros)\n",
      "2017-05-23 19:22:08,304 : INFO : saving TfidfModel object under /home/guest/Documents/git/corpushash/hashed_twitter_tfidf_model, separately None\n",
      "2017-05-23 19:22:08,310 : INFO : saved /home/guest/Documents/git/corpushash/hashed_twitter_tfidf_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 149 ms, sys: 2 ms, total: 151 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(os.path.join(path, 'hashed_twitter_tfidf_model')):\n",
    "    tfidf = models.TfidfModel.load(os.path.join(path, 'hashed_twitter_tfidf_model'))\n",
    "else:\n",
    "    tfidf = models.TfidfModel(mm)\n",
    "    tfidf.save(os.path.join(path, 'hashed_twitter_tfidf_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next step is to train the LSI model with a tfidf transformed corpus. So we will need yet another generator to yield the transformed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tfidf_corpus_stream(corpus):\n",
    "    for doc in corpus:\n",
    "        yield tfidf[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_corpus_s = tfidf_corpus_stream(mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Calculating the LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-23 19:22:16,876 : INFO : using serial LSI version on this node\n",
      "2017-05-23 19:22:16,877 : INFO : updating model with new documents\n",
      "2017-05-23 19:22:17,307 : INFO : preparing a new chunk of documents\n",
      "2017-05-23 19:22:17,402 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-05-23 19:22:17,403 : INFO : 1st phase: constructing (42532, 200) action matrix\n",
      "2017-05-23 19:22:17,625 : INFO : orthonormalizing (42532, 200) action matrix\n",
      "2017-05-23 19:22:20,461 : INFO : 2nd phase: running dense svd on (200, 20000) matrix\n",
      "2017-05-23 19:22:21,477 : INFO : computing the final decomposition\n",
      "2017-05-23 19:22:21,477 : INFO : keeping 100 factors (discarding 22.970% of energy spectrum)\n",
      "2017-05-23 19:22:21,616 : INFO : processed documents up to #20000\n",
      "2017-05-23 19:22:21,618 : INFO : topic #0(20.207): 0.246*\"jop5|1Q2hSwG`#tZK_2ioV=yOV*@^!wp5COZiM^a\" + 0.207*\"VSJ@Y<?2*Ml5K`*M^`fn%Sl87VeEJn97HZ@J!N)q\" + 0.186*\"!>9)Au<YaJX+{JM7AP?)U5*oajx`V))b?ULH?0Hh\" + 0.174*\"_m}~cvg1HpVIO|bt>dW8=MGs!C8L2kkTlA+-20o%\" + 0.171*\"rz`0vOzhOhzO-?GF{Kc$(I)VMUQV?PiXQq{{5Fi(\" + 0.162*\"S|O*yO;SRK#!^mG4#9BvTvstp?ule#4jQqIO0f9r\" + 0.160*\"Awa0Xh7;*goDCfeMAKFNIiX0&p`cJ;<?BW`=p~|~\" + 0.154*\"hw3&5_FWZPo}LDz#qtpy;WwSS3~`A9c2FKOkHS71\" + 0.146*\"$cDf&dS`#1c3j#!mzon9_#Z%8sOGpD^g^F>ig2U`\" + 0.146*\"fcN}Jib*C?#?RW2fwi07i>g_I-FnZEyk+0`JCNx}\"\n",
      "2017-05-23 19:22:21,619 : INFO : topic #1(17.533): 0.288*\"jop5|1Q2hSwG`#tZK_2ioV=yOV*@^!wp5COZiM^a\" + -0.200*\"VSJ@Y<?2*Ml5K`*M^`fn%Sl87VeEJn97HZ@J!N)q\" + 0.187*\"y7@)NoQ$ZAKBG3IPB*+oYDLlWIKG%fd>r4`bmijj\" + 0.187*\"W8F|kEnf~i|0Asv_oK$6o+wON<k*kq{n-CMBWcP3\" + 0.187*\"VQ<?fB^966{WYp<$@-@BE{;i0^oV5aDBf$@9Gqg;\" + 0.186*\"i{q3`oHAicr>zXO9k<5&rlKk$1^_t|6YgRc)MJD6\" + 0.179*\"WZXg?sOlVXKtuK&OkcXtApWJ?=mJXlQy#)!UQH|m\" + 0.177*\"I(M2AmM>cdK=t!s<KvCvQQGPpX?C^lwM+n#s|1&{\" + 0.174*\"v)$7_rt{Qgr$kkZY6xa@FDt|zJEzd|k{@l?UB*N+\" + 0.174*\"II+>6O(4Mdg@o*jqcc~kqosqTcLrto$*mJMM!V?0\"\n",
      "2017-05-23 19:22:21,621 : INFO : topic #2(14.004): 0.235*\"cHPGlwQkwL`eG*`2s^zaHsHm<1uk@icdoi$<4>Hp\" + 0.209*\"M4N`)#?Chc+Wq_WT8JWb$T+l~gdLV#2x6KNjvCe-\" + 0.200*\"j<oOL8!FI2kXUr!C&u}#q#ipUM4pq%dbICGr(A8%\" + 0.189*\"V~Oq8W4MK;`H0zBuYYYgl2Hm2Ej=0P@9%FUi^%SV\" + 0.174*\"ytf1ocAL66-MWSzmqH$R{1;n})CaDCd=d)IYp_pN\" + 0.170*\"h4f*~#GvS~XF#vP)CJPBXaOdL5;`1x*gcpWJ7^!j\" + 0.167*\"Fn_4Vy@%%Gk^w&K#r%7k?F&!Py{VGgNF$=V*T$Js\" + 0.163*\"%%4-eWp+Us8tCuWjwx=zhMyJhoti86w!d7fr!CpS\" + 0.147*\"O=c=aV7e}=is%ZdS5fUCNuS1G`}MlNm($rc+zs+Q\" + 0.145*\"mc?=u!C|D!Ks+xD@cfb#z)=*1?9mxU2Em#mH<Y)d\"\n",
      "2017-05-23 19:22:21,623 : INFO : topic #3(12.137): 0.543*\"M4N`)#?Chc+Wq_WT8JWb$T+l~gdLV#2x6KNjvCe-\" + 0.331*\"ytf1ocAL66-MWSzmqH$R{1;n})CaDCd=d)IYp_pN\" + 0.317*\"cHPGlwQkwL`eG*`2s^zaHsHm<1uk@icdoi$<4>Hp\" + 0.251*\"O=c=aV7e}=is%ZdS5fUCNuS1G`}MlNm($rc+zs+Q\" + 0.223*\"%v2#)%zt#pP*i?4b$0+pY`1L0iZ?7^%PnKs3T#bw\" + 0.124*\"u_0rxX(4V7ypzLS-_-AZXZI?{lImsAkLwU67(46+\" + 0.116*\"`BDb9+?<~a90w$TyeczDG8BS6?{k<AkZqHd0Nlj4\" + 0.115*\"tN{PLVR5w&@n{Bwmcn@Ndt|=(;`&ir)lY6fB2tE>\" + -0.114*\"V~Oq8W4MK;`H0zBuYYYgl2Hm2Ej=0P@9%FUi^%SV\" + -0.113*\"Fn_4Vy@%%Gk^w&K#r%7k?F&!Py{VGgNF$=V*T$Js\"\n",
      "2017-05-23 19:22:21,625 : INFO : topic #4(10.230): -0.424*\"j<oOL8!FI2kXUr!C&u}#q#ipUM4pq%dbICGr(A8%\" + -0.378*\"cHPGlwQkwL`eG*`2s^zaHsHm<1uk@icdoi$<4>Hp\" + 0.216*\"M4N`)#?Chc+Wq_WT8JWb$T+l~gdLV#2x6KNjvCe-\" + -0.204*\"h4f*~#GvS~XF#vP)CJPBXaOdL5;`1x*gcpWJ7^!j\" + 0.182*\"xLp;4gB@hanVFuI^&N%f#JEpd!!96tScpe5_D>71\" + -0.160*\"JlCS+<Fu0($)@VE9Gi4WjYX3h5MOo~6!}2P>wkRq\" + -0.158*\"6CjI0L{itdS-t{97mcxnT}Q*k+?#nx`eA|-%RHnz\" + 0.126*\"ZQZFu73%b~h%WuD_D<RH=Ny)yKH?&WoN`(+*BVm=\" + -0.119*\"V~Oq8W4MK;`H0zBuYYYgl2Hm2Ej=0P@9%FUi^%SV\" + 0.117*\"NkC0aF5_wer~&rX9B#<VHBr=DGJ{n8X*uS4l5iuy\"\n",
      "2017-05-23 19:22:21,897 : INFO : preparing a new chunk of documents\n",
      "2017-05-23 19:22:21,955 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-05-23 19:22:21,956 : INFO : 1st phase: constructing (42532, 200) action matrix\n",
      "2017-05-23 19:22:22,089 : INFO : orthonormalizing (42532, 200) action matrix\n",
      "2017-05-23 19:22:25,337 : INFO : 2nd phase: running dense svd on (200, 10000) matrix\n",
      "2017-05-23 19:22:25,693 : INFO : computing the final decomposition\n",
      "2017-05-23 19:22:25,693 : INFO : keeping 100 factors (discarding 23.851% of energy spectrum)\n",
      "2017-05-23 19:22:25,754 : INFO : merging projections: (42532, 100) + (42532, 100)\n",
      "2017-05-23 19:22:26,389 : INFO : keeping 100 factors (discarding 12.573% of energy spectrum)\n",
      "2017-05-23 19:22:26,485 : INFO : processed documents up to #30000\n",
      "2017-05-23 19:22:26,488 : INFO : topic #0(26.147): 0.313*\"jop5|1Q2hSwG`#tZK_2ioV=yOV*@^!wp5COZiM^a\" + 0.181*\"!>9)Au<YaJX+{JM7AP?)U5*oajx`V))b?ULH?0Hh\" + 0.171*\"y7@)NoQ$ZAKBG3IPB*+oYDLlWIKG%fd>r4`bmijj\" + 0.171*\"VQ<?fB^966{WYp<$@-@BE{;i0^oV5aDBf$@9Gqg;\" + 0.171*\"W8F|kEnf~i|0Asv_oK$6o+wON<k*kq{n-CMBWcP3\" + 0.171*\"i{q3`oHAicr>zXO9k<5&rlKk$1^_t|6YgRc)MJD6\" + 0.167*\"_m}~cvg1HpVIO|bt>dW8=MGs!C8L2kkTlA+-20o%\" + 0.167*\"WZXg?sOlVXKtuK&OkcXtApWJ?=mJXlQy#)!UQH|m\" + 0.166*\"II+>6O(4Mdg@o*jqcc~kqosqTcLrto$*mJMM!V?0\" + 0.164*\"I(M2AmM>cdK=t!s<KvCvQQGPpX?C^lwM+n#s|1&{\"\n",
      "2017-05-23 19:22:26,490 : INFO : topic #1(22.136): 0.247*\"VSJ@Y<?2*Ml5K`*M^`fn%Sl87VeEJn97HZ@J!N)q\" + -0.210*\"jop5|1Q2hSwG`#tZK_2ioV=yOV*@^!wp5COZiM^a\" + 0.178*\"$cDf&dS`#1c3j#!mzon9_#Z%8sOGpD^g^F>ig2U`\" + 0.177*\"Awa0Xh7;*goDCfeMAKFNIiX0&p`cJ;<?BW`=p~|~\" + 0.176*\"tFs$*(PIS0H4!V?0nxRahRi9+iDL6Mnu4(>q#q16\" + 0.176*\"OywintK<PXCAvm(0G=L(e@syXK?mLn{p#9k(EpT*\" + 0.175*\"hd#ju#%~)xjpb|*qs@)e!XW(`-dO8qoh$ArU2JGl\" + 0.172*\"pEFepgf!_Qq@#HJcIu3(2>*&+Uxr3@pMV*EqFeZp\" + 0.172*\"E1u=<FK$s<>A+l~`zn{=`~?|X==fN0Tbt{awm~^Y\" + 0.170*\"*Yb}>fSfz$?hm^8y6|lBP#h|9gI1JEpxf20kTC#b\"\n",
      "2017-05-23 19:22:26,493 : INFO : topic #2(17.285): 0.194*\"%%4-eWp+Us8tCuWjwx=zhMyJhoti86w!d7fr!CpS\" + 0.175*\"VOCc0<g{;I!E27Fi%b7rtwBrL7vO1q^F^SlUk%pY\" + -0.169*\"tFs$*(PIS0H4!V?0nxRahRi9+iDL6Mnu4(>q#q16\" + -0.169*\"OywintK<PXCAvm(0G=L(e@syXK?mLn{p#9k(EpT*\" + -0.168*\"hd#ju#%~)xjpb|*qs@)e!XW(`-dO8qoh$ArU2JGl\" + -0.168*\"$cDf&dS`#1c3j#!mzon9_#Z%8sOGpD^g^F>ig2U`\" + 0.159*\"mc?=u!C|D!Ks+xD@cfb#z)=*1?9mxU2Em#mH<Y)d\" + 0.159*\"M4N`)#?Chc+Wq_WT8JWb$T+l~gdLV#2x6KNjvCe-\" + 0.148*\"V~Oq8W4MK;`H0zBuYYYgl2Hm2Ej=0P@9%FUi^%SV\" + -0.147*\"_&%2QSNq}I_7Vb6_EJItxMOTj{Xffe^iVvG!i-wr\"\n",
      "2017-05-23 19:22:26,496 : INFO : topic #3(14.181): -0.646*\"M4N`)#?Chc+Wq_WT8JWb$T+l~gdLV#2x6KNjvCe-\" + -0.319*\"ytf1ocAL66-MWSzmqH$R{1;n})CaDCd=d)IYp_pN\" + -0.266*\"cHPGlwQkwL`eG*`2s^zaHsHm<1uk@icdoi$<4>Hp\" + -0.220*\"O=c=aV7e}=is%ZdS5fUCNuS1G`}MlNm($rc+zs+Q\" + -0.195*\"%v2#)%zt#pP*i?4b$0+pY`1L0iZ?7^%PnKs3T#bw\" + -0.131*\"`BDb9+?<~a90w$TyeczDG8BS6?{k<AkZqHd0Nlj4\" + -0.130*\"tN{PLVR5w&@n{Bwmcn@Ndt|=(;`&ir)lY6fB2tE>\" + -0.127*\"Gr&?0tzj+PE`aNjlP?CCW4KIz(O=P>BeO=PLxI2f\" + -0.126*\"9;*r%$aJgQWRSWvD7qgMtI)Tj>GO-QVE{eWRsA$d\" + -0.125*\"ch=5b<<69tTPihiP>N4tqN@e1rRXD1y-y;5$?8IW\"\n",
      "2017-05-23 19:22:26,498 : INFO : topic #4(11.743): -0.236*\"jX=1Z5Ih1lNRD-VvoAD|t^%W_%-7KIS+;KsV+I2(\" + -0.236*\"?%%IduW#ZgEB-=57p^>Ncr9EL0I>W^($u_wh$?Js\" + -0.236*\"PR02jy%E+g)+Aen55eK?al1c3qcaJx|8p^lAX`E_\" + -0.236*\"ONs8l%n7uoHMX+U;ih36sXJ_bHbvUW15-Pj&vsfQ\" + -0.235*\"fr%8j+}?sN!|S&&hvR~=Eh0Z(8-0T+WY!7vDYJ|R\" + -0.234*\"rcbZN@i_jV5wc`n24hUO&))U-h2lsPx*c7nNSSy0\" + -0.233*\"B0lzhRWbZ+dpBMU{`HD>uliAVn?b%U!*2Z{i|z=0\" + -0.233*\"gVfln8{)P4ipdurv-Ob=m{OJ<AEru#rOPGC2JGQg\" + -0.225*\"_AQ+FP&GPDalcMWen&#n6jH!EZmHwT9_+{|>p)?G\" + -0.225*\"Ct^=q47vn(2+yQ`Al@S1ARKvV_cfLH@HZ~f%xRUt\"\n",
      "2017-05-23 19:22:26,511 : INFO : saving Projection object under /home/guest/Documents/git/corpushash/hashed_twitter_lsi_model.projection, separately None\n",
      "2017-05-23 19:22:26,702 : INFO : saved /home/guest/Documents/git/corpushash/hashed_twitter_lsi_model.projection\n",
      "2017-05-23 19:22:26,705 : INFO : saving LsiModel object under /home/guest/Documents/git/corpushash/hashed_twitter_lsi_model, separately None\n",
      "2017-05-23 19:22:26,705 : INFO : not storing attribute projection\n",
      "2017-05-23 19:22:26,708 : INFO : not storing attribute dispatcher\n",
      "2017-05-23 19:22:26,726 : INFO : saved /home/guest/Documents/git/corpushash/hashed_twitter_lsi_model\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(path, 'hashed_twitter_lsi_model')):\n",
    "    lsih = gensim.models.LsiModel.load(os.path.join(path, 'hashed_twitter_lsi_model'))\n",
    "else:\n",
    "    lsih = gensim.models.lsimodel.LsiModel(corpus=tfidf_corpus_s, id2word=id2word, num_topics=100)\n",
    "    lsih.save(os.path.join(path, 'hashed_twitter_lsi_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let now look at the topics generated, decoding the hashed tokens using the `decode_dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Topic 0:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.313\t\"\n",
      "0.181\tTories\n",
      "0.171\tpreoccupied\n",
      "0.171\tinequality\n",
      "0.171\t@Tommy_Colc\n",
      "0.171\twrote\n",
      "0.167\tMiliband\n",
      "0.167\tclaiming\n",
      "0.166\tw\n",
      "0.164\tman\n",
      "====================\n",
      "Topic 1:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.247\tSNP\n",
      "-0.21\t\"\n",
      "0.178\tSco\n",
      "0.177\tto\n",
      "0.176\tprotect\n",
      "0.176\tlots\n",
      "0.175\tdefinitely\n",
      "0.172\t@NicolaSturgeon\n",
      "0.172\trather\n",
      "0.17\tlet\n",
      "====================\n",
      "Topic 2:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.194\tthe\n",
      "0.175\t.\n",
      "-0.169\tprotect\n",
      "-0.169\tlots\n",
      "-0.168\tdefinitely\n",
      "-0.168\tSco\n",
      "0.159\ta\n",
      "0.159\t%\n",
      "0.148\tI\n",
      "-0.147\tMPs\n",
      "====================\n",
      "Topic 3:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.646\t%\n",
      "-0.319\t-\n",
      "-0.266\t(\n",
      "-0.22\t)\n",
      "-0.195\t1\n",
      "-0.131\tCON\n",
      "-0.13\tLAB\n",
      "-0.127\tpoll\n",
      "-0.126\t8\n",
      "-0.125\t34\n",
      "====================\n",
      "Topic 4:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.236\tthus\n",
      "-0.236\tahem\n",
      "-0.236\t@thomasmessenger\n",
      "-0.236\thttp://t.co/DkLwCwzhDA\n",
      "-0.235\tfinancial\n",
      "-0.234\tcaused\n",
      "-0.233\tglobal\n",
      "-0.233\tcrisis\n",
      "-0.225\tFor\n",
      "-0.225\toverspent\n",
      "====================\n",
      "Topic 5:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.341\tFT\n",
      "0.321\t(\n",
      "-0.232\t%\n",
      "0.2\t)\n",
      "0.19\t:(\n",
      "0.177\tJonathan\n",
      "0.177\tFord\n",
      "0.177\twriter\n",
      "0.176\tBoris\n",
      "-0.162\t'\n",
      "====================\n",
      "Topic 6:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.414\t'\n",
      "0.182\tdeal\n",
      "-0.171\tCameron\n",
      "-0.162\tDavid\n",
      "0.152\tTomorrow\n",
      "0.147\tmyself\n",
      "0.147\t@mrmarksteel\n",
      "0.145\tcase\n",
      "-0.141\ton\n",
      "0.136\ttell\n",
      "====================\n",
      "Topic 7:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.282\t!\n",
      "0.279\t:(\n",
      "-0.232\tFT\n",
      "0.187\t:)\n",
      "0.182\tyou\n",
      "0.171\tI\n",
      "-0.155\tleader\n",
      "-0.127\t'\n",
      "-0.123\tFord\n",
      "-0.123\tJonathan\n",
      "====================\n",
      "Topic 8:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.312\t'\n",
      "0.24\t-\n",
      "0.21\t(\n",
      "0.197\tLabour\n",
      "-0.173\t,\n",
      "-0.162\t%\n",
      "0.146\t\"\n",
      "0.133\tSNP\n",
      "0.13\twith\n",
      "-0.13\tFT\n",
      "====================\n",
      "Topic 9:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.365\t(\n",
      "0.264\tDavid\n",
      "0.248\tCameron\n",
      "0.202\t:(\n",
      "-0.181\t%\n",
      "0.151\t...\n",
      "-0.144\t#AskNigelFarage\n",
      "0.137\t-\n",
      "0.13\t)\n",
      "0.129\t*\n",
      "====================\n",
      "Topic 10:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.356\t!\n",
      "-0.224\tyou\n",
      "-0.198\the'd\n",
      "-0.185\tthan\n",
      "0.182\t\"\n",
      "-0.182\trather\n",
      "-0.171\t:)\n",
      "0.153\tno\n",
      "-0.139\tlet\n",
      "0.134\t(\n",
      "====================\n",
      "Topic 11:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.527\t\"\n",
      "-0.367\t!\n",
      "0.218\t:(\n",
      "-0.134\t:)\n",
      "0.129\tI\n",
      "0.126\t.\n",
      "-0.121\t-\n",
      "0.0924\t(\n",
      "0.0896\tTories\n",
      "-0.0893\t'\n",
      "====================\n",
      "Topic 12:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.394\t'\n",
      "-0.262\tretweet\n",
      "-0.198\tnot\n",
      "-0.194\tthis\n",
      "-0.165\tdo\n",
      "0.163\t(\n",
      "-0.156\t@LabourEoin\n",
      "-0.138\twould\n",
      "-0.133\thttp://t.co/5D2pKCstr3\n",
      "-0.131\trepeat\n",
      "====================\n",
      "Topic 13:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.527\t\"\n",
      "-0.278\t!\n",
      "-0.201\t'\n",
      "-0.156\t-\n",
      "0.15\tnot\n",
      "-0.143\thttp\n",
      "-0.126\t*\n",
      "0.115\tsays\n",
      "0.106\tdo\n",
      "0.103\tthan\n",
      "====================\n",
      "Topic 14:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.186\treliant\n",
      "-0.186\thungrier\n",
      "0.184\tEd\n",
      "-0.182\tfive\n",
      "-0.178\tbanks\n",
      "-0.176\tfood\n",
      "-0.176\t@Markfergusonuk\n",
      "0.174\t*\n",
      "-0.17\tago\n",
      "-0.164\tyears\n",
      "====================\n",
      "Topic 15:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "-0.26\t#AskNigelFarage\n",
      "-0.206\tFarage\n",
      "0.198\t\"\n",
      "-0.18\t(\n",
      "-0.18\tretweet\n",
      "-0.162\tNigel\n",
      "-0.16\t@Nigel_Farage\n",
      "-0.16\t@UKIP\n",
      "0.157\t:)\n",
      "-0.155\t#UKIP\n",
      "====================\n",
      "Topic 16:\n",
      "Coef.\t Token\n",
      "--------------------\n",
      "0.237\t:(\n",
      "0.162\tEd\n",
      "-0.161\t-\n",
      "-0.16\t)\n",
      "0.144\t%\n",
      "-0.14\twill\n",
      "-0.137\t#bbcqt\n",
      "-0.134\t#AskNigelFarage\n",
      "-0.131\tchild\n",
      "-0.13\t*\n"
     ]
    }
   ],
   "source": [
    "for n in range(17):\n",
    "    print(\"====================\")\n",
    "    print(\"Topic {}:\".format(n))\n",
    "    print(\"Coef.\\t Token\")\n",
    "    print(\"--------------------\")\n",
    "    for tok,coef in lsih.show_topic(n):\n",
    "        tok = hashed.decode_dictionary[tok.strip()][0]\n",
    "        print(\"{:.3}\\t{}\".format(coef,tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### comparing the resulting topics we see that the NLP's results are the same regardless of which corpus we use, i.e., we can use hashed corpora to perform NLP tasks in a lossless manner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
